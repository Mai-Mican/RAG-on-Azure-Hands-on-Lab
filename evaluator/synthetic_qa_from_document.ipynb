{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure AI Document Intelligenceを使用して複雑なPDFからQnA合成データセットを生成する\n",
    "\n",
    "### 概要\n",
    "PDFを3つの部分に分けて処理します。\n",
    "\n",
    "- **混合ページ（画像とテキストが適切に混在）** - Azure AI Document Intelligenceでドキュメントを読み取った後、図のタグ内の画像説明をマルチモーダルLLMで要約されたテキストに置き換えます。（しばしば画像説明は空白か短いキャプションのみです。）\n",
    "- **Text-heavy** - テキストが多いPDFは、Azure AI Document IntelligenceやUnstructuredのようなツールキットを使用せずに、オープンソースで処理できます。\n",
    "- **Image-heavy** - 画像が多いPDFは、ページ全体を画像に変換し、GPT-4oのようなマルチモーダルLLMに各ページを要約させます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 2222k  100 2222k    0     0  7932k      0 --:--:-- --:--:-- --:--:-- 7937k\n",
      "Collecting pip\n",
      "  Using cached pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Using cached pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 24.3.1\n",
      "    Uninstalling pip-24.3.1:\n",
      "      Successfully uninstalled pip-24.3.1\n",
      "Successfully installed pip-24.3.1\n"
     ]
    }
   ],
   "source": [
    "! curl https://bootstrap.pypa.io/get-pip.py -o get-pip.py\n",
    "! python get-pip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "以下の環境変数をセット後 VS Code を再起動\n",
    "- PATH: `C:\\Program Files\\Python312\\Scripts`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pdf2image==1.17.0 (from -r requirements.txt (line 1))\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain==0.2.10 (from -r requirements.txt (line 2))\n",
      "  Downloading langchain-0.2.10-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting langchain-openai==0.1.17 (from -r requirements.txt (line 3))\n",
      "  Downloading langchain_openai-0.1.17-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting langchain-community==0.2.9 (from -r requirements.txt (line 4))\n",
      "  Downloading langchain_community-0.2.9-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: python-dotenv==1.0.1 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from -r requirements.txt (line 5)) (1.0.1)\n",
      "Collecting openai==1.57.3 (from -r requirements.txt (line 6))\n",
      "  Downloading openai-1.57.3-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting transformers~=4.42 (from -r requirements.txt (line 7))\n",
      "  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: pandas~=2.2 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from -r requirements.txt (line 8)) (2.2.3)\n",
      "Collecting tiktoken==0.7.0 (from -r requirements.txt (line 9))\n",
      "  Downloading tiktoken-0.7.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting azure-identity==1.17.1 (from -r requirements.txt (line 10))\n",
      "  Downloading azure_identity-1.17.1-py3-none-any.whl.metadata (79 kB)\n",
      "Collecting azure-search-documents==11.6.0b4 (from -r requirements.txt (line 11))\n",
      "  Downloading azure_search_documents-11.6.0b4-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting azure-ai-generative==1.0.0b8 (from -r requirements.txt (line 12))\n",
      "  Downloading azure_ai_generative-1.0.0b8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting azure-cognitiveservices-speech==1.38.0 (from -r requirements.txt (line 13))\n",
      "  Downloading azure_cognitiveservices_speech-1.38.0-py3-none-macosx_11_0_arm64.whl.metadata (1.5 kB)\n",
      "Collecting azure-ai-documentintelligence==1.0.0b4 (from -r requirements.txt (line 14))\n",
      "  Downloading azure_ai_documentintelligence-1.0.0b4-py3-none-any.whl.metadata (48 kB)\n",
      "Collecting scikit-learn==1.5.1 (from -r requirements.txt (line 15))\n",
      "  Downloading scikit_learn-1.5.1-cp312-cp312-macosx_12_0_arm64.whl.metadata (12 kB)\n",
      "Collecting datasets~=2.20 (from -r requirements.txt (line 16))\n",
      "  Downloading datasets-2.21.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting opencv-python-headless==4.10.0.84 (from -r requirements.txt (line 17))\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting markdownify==0.12.1 (from -r requirements.txt (line 18))\n",
      "  Downloading markdownify-0.12.1-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting pymupdf==1.24.7 (from -r requirements.txt (line 19))\n",
      "  Downloading PyMuPDF-1.24.7-cp312-none-macosx_11_0_arm64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: Jinja2 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from -r requirements.txt (line 20)) (3.1.5)\n",
      "Collecting jsonlines (from -r requirements.txt (line 21))\n",
      "  Downloading jsonlines-4.0.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting setuptools (from -r requirements.txt (line 22))\n",
      "  Downloading setuptools-75.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: pillow in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from pdf2image==1.17.0->-r requirements.txt (line 1)) (10.4.0)\n",
      "Collecting PyYAML>=5.3 (from langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from langchain==0.2.10->-r requirements.txt (line 2)) (2.0.36)\n",
      "Collecting aiohttp<4.0.0,>=3.8.3 (from langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading aiohttp-3.11.11-cp312-cp312-macosx_11_0_arm64.whl.metadata (7.7 kB)\n",
      "Collecting langchain-core<0.3.0,>=0.2.22 (from langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading langchain_core-0.2.43-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting langchain-text-splitters<0.3.0,>=0.2.0 (from langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading langchain_text_splitters-0.2.4-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting langsmith<0.2.0,>=0.1.17 (from langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading langsmith-0.1.147-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting numpy<2.0.0,>=1.26.0 (from langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from langchain==0.2.10->-r requirements.txt (line 2)) (2.10.4)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from langchain==0.2.10->-r requirements.txt (line 2)) (2.32.3)\n",
      "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community==0.2.9->-r requirements.txt (line 4))\n",
      "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from openai==1.57.3->-r requirements.txt (line 6)) (4.7.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from openai==1.57.3->-r requirements.txt (line 6)) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from openai==1.57.3->-r requirements.txt (line 6)) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from openai==1.57.3->-r requirements.txt (line 6)) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from openai==1.57.3->-r requirements.txt (line 6)) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from openai==1.57.3->-r requirements.txt (line 6)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from openai==1.57.3->-r requirements.txt (line 6)) (4.12.2)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from tiktoken==0.7.0->-r requirements.txt (line 9)) (2024.11.6)\n",
      "Requirement already satisfied: azure-core>=1.23.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from azure-identity==1.17.1->-r requirements.txt (line 10)) (1.32.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from azure-identity==1.17.1->-r requirements.txt (line 10)) (44.0.0)\n",
      "Requirement already satisfied: msal>=1.24.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from azure-identity==1.17.1->-r requirements.txt (line 10)) (1.31.1)\n",
      "Requirement already satisfied: msal-extensions>=0.3.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from azure-identity==1.17.1->-r requirements.txt (line 10)) (1.2.0)\n",
      "Collecting azure-common>=1.1 (from azure-search-documents==11.6.0b4->-r requirements.txt (line 11))\n",
      "  Downloading azure_common-1.1.28-py2.py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: isodate>=0.6.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from azure-search-documents==11.6.0b4->-r requirements.txt (line 11)) (0.7.2)\n",
      "Collecting azure-ai-resources>=1.0.0b7 (from azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading azure_ai_resources-1.0.0b8-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting mlflow-skinny<3,>=1.27.0 (from azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading mlflow_skinny-2.19.0-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting opencensus-ext-azure~=1.0 (from azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading opencensus_ext_azure-1.1.13-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting opencensus-ext-logging<=0.1.1 (from azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading opencensus_ext_logging-0.1.1-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn==1.5.1->-r requirements.txt (line 15))\n",
      "  Downloading scipy-1.15.0-cp312-cp312-macosx_14_0_arm64.whl.metadata (61 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from scikit-learn==1.5.1->-r requirements.txt (line 15)) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.5.1->-r requirements.txt (line 15))\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting beautifulsoup4<5,>=4.9 (from markdownify==0.12.1->-r requirements.txt (line 18))\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: six<2,>=1.15 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from markdownify==0.12.1->-r requirements.txt (line 18)) (1.17.0)\n",
      "Collecting PyMuPDFb==1.24.6 (from pymupdf==1.24.7->-r requirements.txt (line 19))\n",
      "  Downloading PyMuPDFb-1.24.6-py3-none-macosx_11_0_arm64.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: filelock in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from transformers~=4.42->-r requirements.txt (line 7)) (3.16.1)\n",
      "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers~=4.42->-r requirements.txt (line 7))\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from transformers~=4.42->-r requirements.txt (line 7)) (24.2)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers~=4.42->-r requirements.txt (line 7))\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers~=4.42->-r requirements.txt (line 7))\n",
      "  Downloading safetensors-0.5.0-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from pandas~=2.2->-r requirements.txt (line 8)) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from pandas~=2.2->-r requirements.txt (line 8)) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from pandas~=2.2->-r requirements.txt (line 8)) (2024.2)\n",
      "Collecting pyarrow>=15.0.0 (from datasets~=2.20->-r requirements.txt (line 16))\n",
      "  Downloading pyarrow-18.1.0-cp312-cp312-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets~=2.20->-r requirements.txt (line 16))\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting xxhash (from datasets~=2.20->-r requirements.txt (line 16))\n",
      "  Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Collecting multiprocess (from datasets~=2.20->-r requirements.txt (line 16))\n",
      "  Downloading multiprocess-0.70.17-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.6.1,>=2023.1.0 (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets~=2.20->-r requirements.txt (line 16))\n",
      "  Downloading fsspec-2024.6.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from Jinja2->-r requirements.txt (line 20)) (3.0.2)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from jsonlines->-r requirements.txt (line 21)) (24.3.0)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading frozenlist-1.5.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (13 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (5.0 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading propcache-0.2.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (9.2 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp<4.0.0,>=3.8.3->langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading yarl-1.18.3-cp312-cp312-macosx_11_0_arm64.whl.metadata (69 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai==1.57.3->-r requirements.txt (line 6)) (3.10)\n",
      "Collecting azure-ai-ml>=1.14.0 (from azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading azure_ai_ml-1.23.0-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting azure-mgmt-resource<23.0.0,>=22.0.0 (from azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading azure_mgmt_resource-22.0.0-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4<5,>=4.9->markdownify==0.12.1->-r requirements.txt (line 18))\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from cryptography>=2.5->azure-identity==1.17.1->-r requirements.txt (line 10)) (1.17.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.9->-r requirements.txt (line 4)) (3.23.2)\n",
      "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.9->-r requirements.txt (line 4))\n",
      "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: certifi in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai==1.57.3->-r requirements.txt (line 6)) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai==1.57.3->-r requirements.txt (line 6)) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai==1.57.3->-r requirements.txt (line 6)) (0.14.0)\n",
      "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.3.0,>=0.2.22->langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading orjson-3.10.13-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Collecting cachetools<6,>=5.0.0 (from mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: click<9,>=7.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (8.1.8)\n",
      "Collecting cloudpickle<4 (from mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading cloudpickle-3.1.0-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading databricks_sdk-0.40.0-py3-none-any.whl.metadata (38 kB)\n",
      "Requirement already satisfied: gitpython<4,>=3.1.9 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (3.1.43)\n",
      "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (8.5.0)\n",
      "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (1.29.0)\n",
      "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (1.29.0)\n",
      "Requirement already satisfied: protobuf<6,>=3.12.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (5.29.2)\n",
      "Collecting sqlparse<1,>=0.4.0 (from mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading sqlparse-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: PyJWT<3,>=1.0.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal>=1.24.0->azure-identity==1.17.1->-r requirements.txt (line 10)) (2.10.1)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from msal-extensions>=0.3.0->azure-identity==1.17.1->-r requirements.txt (line 10)) (2.10.1)\n",
      "Collecting opencensus<1.0.0,>=0.11.4 (from opencensus-ext-azure~=1.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: psutil>=5.6.3 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from opencensus-ext-azure~=1.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (5.9.8)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from pydantic<3,>=1->langchain==0.2.10->-r requirements.txt (line 2)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from pydantic<3,>=1->langchain==0.2.10->-r requirements.txt (line 2)) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.2.10->-r requirements.txt (line 2)) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from requests<3,>=2->langchain==0.2.10->-r requirements.txt (line 2)) (2.3.0)\n",
      "INFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting multiprocess (from datasets~=2.20->-r requirements.txt (line 16))\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: msrest>=0.6.18 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (0.7.1)\n",
      "Collecting azure-mgmt-core>=1.3.0 (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading azure_mgmt_core-1.5.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: jsonschema>=4.0.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (4.23.0)\n",
      "Requirement already satisfied: strictyaml in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (1.7.3)\n",
      "Requirement already satisfied: colorama in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (0.4.6)\n",
      "Collecting azure-storage-blob>=12.10.0 (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading azure_storage_blob-12.24.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting azure-storage-file-share (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading azure_storage_file_share-12.20.0-py3-none-any.whl.metadata (49 kB)\n",
      "Collecting azure-storage-file-datalake>=12.2.0 (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading azure_storage_file_datalake-12.18.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: pydash>=6.0.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (7.0.7)\n",
      "Requirement already satisfied: pycparser in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.5->azure-identity==1.17.1->-r requirements.txt (line 10)) (2.22)\n",
      "Collecting google-auth~=2.0 (from databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading google_auth-2.37.0-py2.py3-none-any.whl.metadata (4.8 kB)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from gitpython<4,>=3.1.9->mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (4.0.11)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (3.21.0)\n",
      "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.22->langchain==0.2.10->-r requirements.txt (line 2))\n",
      "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opencensus-context>=0.1.3 (from opencensus<1.0.0,>=0.11.4->opencensus-ext-azure~=1.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-api-core<3.0.0,>=1.0.0 (from opencensus<1.0.0,>=0.11.4->opencensus-ext-azure~=1.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading google_api_core-2.24.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (1.2.15)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.50b0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (0.50b0)\n",
      "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community==0.2.9->-r requirements.txt (line 4))\n",
      "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: wrapt<2,>=1.10 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (1.17.0)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (5.0.1)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure~=1.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (1.66.0)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure~=1.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading proto_plus-1.25.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from jsonschema>=4.0.0->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (2024.10.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from jsonschema>=4.0.0->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from jsonschema>=4.0.0->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (0.22.3)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from msrest>=0.6.18->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (2.0.0)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny<3,>=1.27.0->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12))\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/matsu/.pyenv/versions/3.12.8/lib/python3.12/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-ai-ml>=1.14.0->azure-ai-resources>=1.0.0b7->azure-ai-generative==1.0.0b8->-r requirements.txt (line 12)) (3.2.2)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading langchain-0.2.10-py3-none-any.whl (990 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.0/990.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading langchain_openai-0.1.17-py3-none-any.whl (46 kB)\n",
      "Downloading langchain_community-0.2.9-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.57.3-py3-none-any.whl (390 kB)\n",
      "Downloading tiktoken-0.7.0-cp312-cp312-macosx_11_0_arm64.whl (906 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m906.7/906.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading azure_identity-1.17.1-py3-none-any.whl (173 kB)\n",
      "Downloading azure_search_documents-11.6.0b4-py3-none-any.whl (325 kB)\n",
      "Downloading azure_ai_generative-1.0.0b8-py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading azure_cognitiveservices_speech-1.38.0-py3-none-macosx_11_0_arm64.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading azure_ai_documentintelligence-1.0.0b4-py3-none-any.whl (99 kB)\n",
      "Downloading scikit_learn-1.5.1-cp312-cp312-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 MB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading markdownify-0.12.1-py3-none-any.whl (16 kB)\n",
      "Downloading PyMuPDF-1.24.7-cp312-none-macosx_11_0_arm64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading PyMuPDFb-1.24.6-py3-none-macosx_11_0_arm64.whl (14.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading transformers-4.47.1-py3-none-any.whl (10.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.21.0-py3-none-any.whl (527 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
      "Downloading setuptools-75.7.0-py3-none-any.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m-:--:--\u001b[0m\n",
      "\u001b[?25hDownloading aiohttp-3.11.11-cp312-cp312-macosx_11_0_arm64.whl (455 kB)\n",
      "Downloading azure_ai_resources-1.0.0b8-py3-none-any.whl (2.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mmm\n",
      "\u001b[?25hDownloading azure_common-1.1.28-py2.py3-none-any.whl (14 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.6.1-py3-none-any.whl (177 kB)\n",
      "Downloading huggingface_hub-0.27.1-py3-none-any.whl (450 kB)\n",
      "Downloading langchain_core-0.2.43-py3-none-any.whl (397 kB)\n",
      "Downloading langchain_text_splitters-0.2.4-py3-none-any.whl (25 kB)\n",
      "Downloading langsmith-0.1.147-py3-none-any.whl (311 kB)\n",
      "Downloading mlflow_skinny-2.19.0-py3-none-any.whl (5.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opencensus_ext_azure-1.1.13-py2.py3-none-any.whl (43 kB)\n",
      "Downloading opencensus_ext_logging-0.1.1-py2.py3-none-any.whl (4.0 kB)\n",
      "Downloading pyarrow-18.1.0-cp312-cp312-macosx_12_0_arm64.whl (29.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.5/29.5 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp312-cp312-macosx_11_0_arm64.whl (173 kB)\n",
      "Downloading safetensors-0.5.0-cp38-abi3-macosx_11_0_arm64.whl (408 kB)\n",
      "Downloading scipy-1.15.0-cp312-cp312-macosx_14_0_arm64.whl (24.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading xxhash-3.5.0-cp312-cp312-macosx_11_0_arm64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.4-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading azure_ai_ml-1.23.0-py3-none-any.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading azure_mgmt_resource-22.0.0-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading cloudpickle-3.1.0-py3-none-any.whl (22 kB)\n",
      "Downloading databricks_sdk-0.40.0-py3-none-any.whl (629 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m629.7/629.7 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frozenlist-1.5.0-cp312-cp312-macosx_11_0_arm64.whl (51 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading multidict-6.1.0-cp312-cp312-macosx_11_0_arm64.whl (29 kB)\n",
      "Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
      "Downloading orjson-3.10.13-cp312-cp312-macosx_10_15_x86_64.macosx_11_0_arm64.macosx_10_15_universal2.whl (248 kB)\n",
      "Downloading propcache-0.2.1-cp312-cp312-macosx_11_0_arm64.whl (45 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Downloading sqlparse-0.5.3-py3-none-any.whl (44 kB)\n",
      "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
      "Downloading yarl-1.18.3-cp312-cp312-macosx_11_0_arm64.whl (92 kB)\n",
      "Downloading azure_mgmt_core-1.5.0-py3-none-any.whl (30 kB)\n",
      "Downloading azure_storage_blob-12.24.0-py3-none-any.whl (408 kB)\n",
      "Downloading azure_storage_file_datalake-12.18.0-py3-none-any.whl (258 kB)\n",
      "Downloading google_api_core-2.24.0-py3-none-any.whl (158 kB)\n",
      "Downloading google_auth-2.37.0-py2.py3-none-any.whl (209 kB)\n",
      "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
      "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
      "Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
      "Downloading azure_storage_file_share-12.20.0-py3-none-any.whl (286 kB)\n",
      "Downloading proto_plus-1.25.0-py3-none-any.whl (50 kB)\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Installing collected packages: opencensus-context, azure-common, xxhash, threadpoolctl, tenacity, sqlparse, soupsieve, setuptools, safetensors, PyYAML, PyMuPDFb, pyasn1, pyarrow, proto-plus, propcache, pdf2image, orjson, numpy, mypy-extensions, multidict, jsonpointer, jsonlines, fsspec, frozenlist, dill, cloudpickle, cachetools, azure-cognitiveservices-speech, aiohappyeyeballs, yarl, typing-inspect, tiktoken, scipy, rsa, requests-toolbelt, pymupdf, pyasn1-modules, opencv-python-headless, multiprocess, jsonpatch, huggingface-hub, beautifulsoup4, aiosignal, tokenizers, scikit-learn, openai, markdownify, langsmith, google-auth, dataclasses-json, azure-storage-file-share, azure-storage-blob, azure-search-documents, azure-mgmt-core, azure-ai-documentintelligence, aiohttp, transformers, langchain-core, google-api-core, databricks-sdk, azure-storage-file-datalake, azure-mgmt-resource, opencensus, mlflow-skinny, langchain-text-splitters, langchain-openai, datasets, opencensus-ext-logging, langchain, azure-identity, opencensus-ext-azure, langchain-community, azure-ai-ml, azure-ai-resources, azure-ai-generative\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.1\n",
      "    Uninstalling numpy-2.2.1:\n",
      "      Successfully uninstalled numpy-2.2.1\n",
      "  Attempting uninstall: tiktoken\n",
      "    Found existing installation: tiktoken 0.8.0\n",
      "    Uninstalling tiktoken-0.8.0:\n",
      "      Successfully uninstalled tiktoken-0.8.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.58.1\n",
      "    Uninstalling openai-1.58.1:\n",
      "      Successfully uninstalled openai-1.58.1\n",
      "  Attempting uninstall: azure-identity\n",
      "    Found existing installation: azure-identity 1.19.0\n",
      "    Uninstalling azure-identity-1.19.0:\n",
      "      Successfully uninstalled azure-identity-1.19.0\n",
      "Successfully installed PyMuPDFb-1.24.6 PyYAML-6.0.2 aiohappyeyeballs-2.4.4 aiohttp-3.11.11 aiosignal-1.3.2 azure-ai-documentintelligence-1.0.0b4 azure-ai-generative-1.0.0b8 azure-ai-ml-1.23.0 azure-ai-resources-1.0.0b8 azure-cognitiveservices-speech-1.38.0 azure-common-1.1.28 azure-identity-1.17.1 azure-mgmt-core-1.5.0 azure-mgmt-resource-22.0.0 azure-search-documents-11.6.0b4 azure-storage-blob-12.24.0 azure-storage-file-datalake-12.18.0 azure-storage-file-share-12.20.0 beautifulsoup4-4.12.3 cachetools-5.5.0 cloudpickle-3.1.0 databricks-sdk-0.40.0 dataclasses-json-0.6.7 datasets-2.21.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.6.1 google-api-core-2.24.0 google-auth-2.37.0 huggingface-hub-0.27.1 jsonlines-4.0.0 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.2.10 langchain-community-0.2.9 langchain-core-0.2.43 langchain-openai-0.1.17 langchain-text-splitters-0.2.4 langsmith-0.1.147 markdownify-0.12.1 mlflow-skinny-2.19.0 multidict-6.1.0 multiprocess-0.70.16 mypy-extensions-1.0.0 numpy-1.26.4 openai-1.57.3 opencensus-0.11.4 opencensus-context-0.1.3 opencensus-ext-azure-1.1.13 opencensus-ext-logging-0.1.1 opencv-python-headless-4.10.0.84 orjson-3.10.13 pdf2image-1.17.0 propcache-0.2.1 proto-plus-1.25.0 pyarrow-18.1.0 pyasn1-0.6.1 pyasn1-modules-0.4.1 pymupdf-1.24.7 requests-toolbelt-1.0.0 rsa-4.9 safetensors-0.5.0 scikit-learn-1.5.1 scipy-1.15.0 setuptools-75.7.0 soupsieve-2.6 sqlparse-0.5.3 tenacity-8.5.0 threadpoolctl-3.5.0 tiktoken-0.7.0 tokenizers-0.21.0 transformers-4.47.1 typing-inspect-0.9.0 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    }
   ],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 事前準備\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Azure Portal から Document intelligence を作成し、エンドポイント・キーをコピー。\n",
    "- Document intelligence のエンドポイント・キー、Azure OpenAI のエンドポイント・キーを環境変数にセット。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.environ[\"AZURE_DOC_INTELLIGENCE_ENDPOINT\"] = \"https://di-cloudworkshop-demo-2.cognitiveservices.azure.com/\"\n",
    "os.environ[\"AZURE_DOC_INTELLIGENCE_KEY\"] = \"33kV5rRJ8t19WtQxxpl3vE9ov6G7Q3am9jWGLp6vJlghKKSEsXi0JQQJ99BAACYeBjFXJ3w3AAALACOGajX1\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://ai-testhub20241201124224481418.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"kAShpx8wgD2ZNHC4LD9Eq3nWhZEFt9FVXeJh9rNtpMoylnxYkTH1JQQJ99ALACHYHv6XJ3w3AAAAACOGMoDC\"\n",
    "os.environ[\"AZURE_OPENAI_API_VERSION\"] = \"2024-08-01-preview\"\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Read & Preprocess PDF file\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDFを個々のページに分割する\n",
    "テストのためにPDFドキュメントの一部のみを使用します"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain: Distributed training on Cloud, Language: Japanese, Language Code: ja\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from util.common_utils import get_language_code\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "raw_data_dir = \"../contents\"\n",
    "splitted_raw_data_dir = \"splitted_contents\"\n",
    "file_path = f\"{raw_data_dir}/azure-ai-search-overview.pdf\"\n",
    "\n",
    "DOMAIN = \"Distributed training on Cloud\"\n",
    "LANGUAGE = \"Japanese\" # You can change your language here. e.g., \"Korean\", \"English\", \"Chinese\"\n",
    "LANGUAGE_CODE = get_language_code(LANGUAGE)\n",
    "print(f\"Domain: {DOMAIN}, Language: {LANGUAGE}, Language Code: {LANGUAGE_CODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "テストのためにPDFドキュメントの一部のみを使用します。ページが多い場合や部分的な処理が必要な場合は、一部のページのみを切り取って保存します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "\n",
    "# Open the first PDF document\n",
    "doc1 = fitz.open(file_path)\n",
    "split_pages = [(5, 25)]\n",
    "\n",
    "for idx, s in enumerate(split_pages):\n",
    "    # Create a new empty PDF document\n",
    "    doc2 = fitz.open()\n",
    "\n",
    "    # Insert the first 2 pages of doc1 into doc2\n",
    "    doc2.insert_pdf(doc1, from_page=s[0], to_page=s[1])\n",
    "\n",
    "    # Save the modified document\n",
    "    doc2.save(f\"{raw_data_dir}/part{idx}.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ファイルエクスプローラーから、作成された `contents/part0.pdf` を確認してみます。PDF の一部が切り取られていることがわかります。\n",
    "\n",
    "![part0.pdf](../images/appendix-01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主にテキストで構成されたページ、主に画像で構成されたページ、およびテキストと画像が混在するページを区別します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'splitted_contents' and its contents have been deleted.\n",
      "### PDF Content Analysis Result:\n",
      "Mixed pages: [0]\n"
     ]
    }
   ],
   "source": [
    "from util.common_utils import delete_folder_and_make_folder\n",
    "from util.preprocess import analyze_pdf_page_content, split_pdf\n",
    "\n",
    "file_path = f\"{raw_data_dir}/part0.pdf\"\n",
    "analyzed_pdf_result = analyze_pdf_page_content(file_path)\n",
    "delete_folder_and_make_folder(splitted_raw_data_dir)    \n",
    "\n",
    "print(\"### PDF Content Analysis Result:\")\n",
    "for content_type, pages in analyzed_pdf_result.items():\n",
    "    print(f\"{content_type} pages: {pages}\")\n",
    "    split_pdf(file_path, f\"{splitted_raw_data_dir}/{content_type}.pdf\", pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import ContentFormat\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "doc_intelligence_endpoint = os.getenv(\"AZURE_DOC_INTELLIGENCE_ENDPOINT\")\n",
    "doc_intelligence_key = os.getenv(\"AZURE_DOC_INTELLIGENCE_KEY\")\n",
    "\n",
    "document_intelligence_client = DocumentIntelligenceClient(\n",
    "    endpoint=doc_intelligence_endpoint, \n",
    "    credential=AzureKeyCredential(doc_intelligence_key),\n",
    "    headers={\"x-ms-useragent\":\"sample-code-figure-understanding/1.0.0\"},\n",
    ")\n",
    "\n",
    "aoai_api_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "aoai_api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "aoai_api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "aoai_deployment_name = os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key=aoai_api_key,  \n",
    "    api_version=aoai_api_version,\n",
    "    base_url=f\"{aoai_api_endpoint}/openai/deployments/{aoai_deployment_name}\",\n",
    "    max_retries=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ケース1: 混合ページ（画像とテキストが適切に混在）の場合\n",
    "Azure AI Document Intelligenceでドキュメントを読み取った後、`img`タグ内の画像説明をマルチモーダルLLMで要約されたテキストに置き換えます。（しばしば画像説明は空白か短いキャプションのみです。）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ドキュメントの分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'pdf_mixed_tmp' and its contents have been deleted.\n"
     ]
    }
   ],
   "source": [
    "if \"Mixed\" in analyzed_pdf_result:\n",
    "    pdf_mixed_path = f\"{splitted_raw_data_dir}/Mixed.pdf\"\n",
    "\n",
    "    with open(pdf_mixed_path, \"rb\") as f:\n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-layout\", analyze_request=f, content_type=\"application/octet-stream\", \n",
    "            output_content_format=ContentFormat.MARKDOWN \n",
    "        )\n",
    "\n",
    "    result = poller.result()\n",
    "    md_content = result.content\n",
    "\n",
    "    #### Updates the content of the figure description (empty content or caption) with the image summary text generated by gpt-4o.\n",
    "    from util.preprocess import (\n",
    "        image_complexity, is_bounding_box_larger_than, crop_image_from_file, \n",
    "        understand_image_with_gpt, update_figure_description\n",
    "    )\n",
    "    output_folder = \"pdf_mixed_tmp\"\n",
    "    delete_folder_and_make_folder(output_folder)\n",
    "    language = LANGUAGE\n",
    "    max_tokens = 1024\n",
    "    input_file_path = file_path\n",
    "\n",
    "    if result.figures:\n",
    "        print(\"Figures:\")\n",
    "        for idx, figure in enumerate(result.figures):\n",
    "            figure_content = \"\"\n",
    "            img_description = \"\"\n",
    "            \n",
    "            for i, span in enumerate(figure.spans):\n",
    "                figure_content += md_content[span.offset:span.offset + span.length]\n",
    "\n",
    "            # Note: figure bounding regions currently contain both the bounding region of figure caption and figure body\n",
    "            if figure.caption:\n",
    "                caption_region = figure.caption.bounding_regions\n",
    "                for region in figure.bounding_regions:\n",
    "                    if region not in caption_region:\n",
    "                        boundingbox = (\n",
    "                                region.polygon[0],  # x0 (left)\n",
    "                                region.polygon[1],  # y0 (top)\n",
    "                                region.polygon[4],  # x1 (right)\n",
    "                                region.polygon[5]   # y1 (bottom)\n",
    "                            )\n",
    "\n",
    "                        if is_bounding_box_larger_than(boundingbox):\n",
    "                            cropped_image = crop_image_from_file(pdf_mixed_path, region.page_number - 1, boundingbox) # page_number is 1-indexed\n",
    "\n",
    "                            if image_complexity(cropped_image)[0] == \"Complex\":\n",
    "                                # Get the base name of the file\n",
    "                                base_name = os.path.basename(input_file_path)\n",
    "                                # Remove the file extension\n",
    "                                file_name_without_extension = os.path.splitext(base_name)[0]\n",
    "\n",
    "                                output_file = f\"{file_name_without_extension}_cropped_image_{idx}.png\"\n",
    "                                cropped_image_filename = os.path.join(output_folder, output_file)\n",
    "\n",
    "                                cropped_image.save(cropped_image_filename)\n",
    "                                print(f\"\\tFigure {idx} cropped and saved as {cropped_image_filename}\")\n",
    "\n",
    "                                try: \n",
    "                                    image_summarization = understand_image_with_gpt(client, aoai_deployment_name, cropped_image_filename, \"\", max_tokens=max_tokens, language=language)\n",
    "                                except openai.BadRequestError as e:\n",
    "                                    print(f\"BadRequestError: {e}\")\n",
    "                                    image_summarization = \"\"\n",
    "                                img_description += image_summarization\n",
    "\n",
    "                                print(f\"\\tDescription of figure {idx}: {img_description}\")\n",
    "                            else:\n",
    "                                print(f'simple image at idx {idx}')\n",
    "\n",
    "            else:\n",
    "                for region in figure.bounding_regions:\n",
    "\n",
    "                    # To learn more about bounding regions, see https://aka.ms/bounding-region\n",
    "                    boundingbox = (\n",
    "                            region.polygon[0],  # x0 (left)\n",
    "                            region.polygon[1],  # y0 (top\n",
    "                            region.polygon[4],  # x1 (right)\n",
    "                            region.polygon[5]   # y1 (bottom)\n",
    "                        )\n",
    "\n",
    "                    if is_bounding_box_larger_than(boundingbox):                    \n",
    "\n",
    "                        cropped_image = crop_image_from_file(input_file_path, region.page_number - 1, boundingbox) # page_number is 1-indexed\n",
    "\n",
    "                        if image_complexity(cropped_image)[0] == \"Complex\":\n",
    "                            # Get the base name of the file\n",
    "                            base_name = os.path.basename(input_file_path)\n",
    "                            # Remove the file extension\n",
    "                            file_name_without_extension = os.path.splitext(base_name)[0]\n",
    "\n",
    "                            output_file = f\"{file_name_without_extension}_cropped_image_{idx}.png\"\n",
    "                            cropped_image_filename = os.path.join(output_folder, output_file)\n",
    "                            # cropped_image_filename = f\"data/cropped/image_{idx}.png\"\n",
    "                            cropped_image.save(cropped_image_filename)\n",
    "\n",
    "                            try:\n",
    "                                image_summarization = understand_image_with_gpt(client, aoai_deployment_name, cropped_image_filename, \"\", max_tokens=max_tokens, language=language)\n",
    "                            except openai.BadRequestError as e:\n",
    "                                print(f\"BadRequestError: {e}\")\n",
    "                                image_summarization = \"\"\n",
    "                            img_description += image_summarization\n",
    "                            print(f\"\\tDescription of figure {idx}: {img_description}\")\n",
    "                        else:\n",
    "                            print(f'simple image at idx {idx}')\n",
    "\n",
    "            \n",
    "            md_content = update_figure_description(md_content, img_description, idx)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 混合ページのチャンクを生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of splits (mixed case): 1\n"
     ]
    }
   ],
   "source": [
    "if \"Mixed\" in analyzed_pdf_result:\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    import re\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\n",
    "            r'<!-- PageNumber=\"\\d+\" -->',\n",
    "            r\"\\n\\n\",\n",
    "            r\"\\n\",\n",
    "            \" \",\n",
    "            \".\",\n",
    "            \"\",\n",
    "        ],   \n",
    "        is_separator_regex = True,    \n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "\n",
    "    mixed_chunks = text_splitter.split_text(md_content)\n",
    "    print(\"Length of splits (mixed case): \" + str(len(mixed_chunks)))\n",
    "else:\n",
    "    mixed_chunks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ケース2: テキストが多い場合\n",
    "テキストが多いPDFは、Azure AI Document IntelligenceやUnstructuredのようなツールキットを使用せずに、オープンソースで処理できます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Text\" in analyzed_pdf_result:\n",
    "    from langchain_community.document_loaders.pdf import PyMuPDFLoader\n",
    "    from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "\n",
    "    pdf_text_path = f\"{splitted_raw_data_dir}/Text.pdf\"\n",
    "    loader = PyMuPDFLoader(pdf_text_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1200, \n",
    "        chunk_overlap=200\n",
    "    )\n",
    "\n",
    "    text_chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    for idx, chunk in enumerate(text_chunks):\n",
    "        print(f\"Chunk {idx}\\n{chunk}\")\n",
    "        print(\"=\"*80)\n",
    "        if idx == 2:\n",
    "            break\n",
    "\n",
    "    text_chunks = [d.page_content for d in text_chunks]\n",
    "    print(\"Length of splits (text-heay case): \" + str(len(text_chunks)))\n",
    "else:\n",
    "    text_chunks = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ケース3: 画像が多い場合\n",
    "画像が多いPDFは、ページ全体を画像に変換し、GPT-4oのようなマルチモーダルLLMに各ページを要約させます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 画像の前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Image\" in analyzed_pdf_result:\n",
    "    import fitz\n",
    "    from glob import glob\n",
    "\n",
    "    image_dir = \"./pdf_image_tmp\"\n",
    "    delete_folder_and_make_folder(image_dir) \n",
    "\n",
    "    pdf_image_path = f\"{splitted_raw_data_dir}/Image.pdf\"\n",
    "    doc = fitz.open(pdf_image_path)\n",
    "    clip_x, clip_y = 10, 10\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        x, y, w, h = page.rect\n",
    "        clip = fitz.Rect(x+clip_x, y+clip_y, w-clip_x, h-clip_y)\n",
    "        page.set_cropbox(clip)\n",
    "        pix = page.get_pixmap()\n",
    "        pix.save(f\"{image_dir}/page_{i:03d}.jpg\")\n",
    "\n",
    "    images = sorted(glob(os.path.join(image_dir, \"*.jpg\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate, HumanMessagePromptTemplate, SystemMessagePromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "max_tokens = 1024\n",
    "llm = AzureChatOpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=max_tokens,\n",
    "    openai_api_version=aoai_api_version,\n",
    "    azure_deployment=aoai_deployment_name                       \n",
    ")\n",
    "\n",
    "human_prompt_main = f\"Given image, give a concise summary in {LANGUAGE}. Don't insert any XML tag such as <text> and </text> when answering.\"\n",
    "\n",
    "system_prompt = \"You are an assistant tasked with describing table or image, specialized in Smartphone product.\"\n",
    "system_message_template = SystemMessagePromptTemplate.from_template(system_prompt)\n",
    "human_prompt = [\n",
    "    {\n",
    "        \"type\": \"image_url\",\n",
    "        \"image_url\": {\n",
    "            \"url\": \"data:image/png;base64,\" + \"{image_base64}\",\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": human_prompt_main\n",
    "    },\n",
    "]\n",
    "human_message_template = HumanMessagePromptTemplate.from_template(human_prompt)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        system_message_template,\n",
    "        human_message_template\n",
    "    ]\n",
    ")\n",
    "\n",
    "summarize_chain = prompt | llm | StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 μs, sys: 1 μs, total: 3 μs\n",
      "Wall time: 4.05 μs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if \"Image\" in analyzed_pdf_result:\n",
    "    from util.preprocess import encode_image_base64\n",
    "    #images = glob(os.path.join(image_path, \"*.jpg\"))\n",
    "    base64_images = [encode_image_base64(img_path) for img_path in images]\n",
    "    image_summaries = summarize_chain.batch(base64_images, {\"max_concurrency\": 8})\n",
    "    image_summaries = remove_short_sentences(image_summaries)\n",
    "    print(\"Length of image_summaries (image-heavy case): \" + str(len(image_summaries)))\n",
    "else:\n",
    "    image_summaries = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Q&Aペアの構築\n",
    "----\n",
    "\n",
    "### オプション1.\n",
    "azure-ai-generativeパッケージを活用します。このパッケージのQADataGeneratorクラスを使用すると、QnAの合成質問を簡単に生成できます。ただし、このクラスをそのまま使用するとカスタムプロンプトを使用できないという欠点があるため、これを継承してCustomQADataGeneratorクラスを作成しました。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.qa import CustomQADataGenerator\n",
    "model_config = {\n",
    "    \"deployment\": os.getenv(\"AZURE_OPENAI_DEPLOYMENT_NAME\"),\n",
    "    \"model\": \"gpt-4o\",\n",
    "    \"max_tokens\": 2000,\n",
    "}\n",
    "\n",
    "qa_generator = CustomQADataGenerator(model_config=model_config, templates_dir=f\"./prompt_template/{LANGUAGE_CODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from collections import Counter\n",
    "from typing import Dict\n",
    "import os\n",
    "from azure.ai.generative.synthetic.qa import QAType\n",
    "concurrency = 6  # number of concurrent calls\n",
    "sem = asyncio.Semaphore(concurrency)\n",
    "\n",
    "#qa_type = QAType.CONVERSATION\n",
    "qa_type = QAType.LONG_ANSWER\n",
    "\n",
    "async def generate_async(text: str) -> Dict:\n",
    "    async with sem:\n",
    "        return await qa_generator.generate_async(\n",
    "            text=text,\n",
    "            qa_type=qa_type,\n",
    "            num_questions=50,  # Number of questions to generate per text\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Parsing error: First line must be a question",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[0;32m----> 7\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m result  \u001b[38;5;66;03m# exception raised inside generate_async()\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     question_answer_list\u001b[38;5;241m.\u001b[39mappend(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquestion_answers\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuccessfully generated QAs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 14\u001b[0m, in \u001b[0;36mgenerate_async\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_async\u001b[39m(text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict:\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m sem:\n\u001b[0;32m---> 14\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m qa_generator\u001b[38;5;241m.\u001b[39mgenerate_async(\n\u001b[1;32m     15\u001b[0m             text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[1;32m     16\u001b[0m             qa_type\u001b[38;5;241m=\u001b[39mqa_type,\n\u001b[1;32m     17\u001b[0m             num_questions\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,  \u001b[38;5;66;03m# Number of questions to generate per text\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/azure/ai/generative/synthetic/qa.py:432\u001b[0m, in \u001b[0;36mQADataGenerator.generate_async\u001b[0;34m(self, text, qa_type, num_questions)\u001b[0m\n\u001b[1;32m    427\u001b[0m validated_num_questions: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m num_questions  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[1;32m    428\u001b[0m content, token_usage \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m _completion_with_retries_async(\n\u001b[1;32m    429\u001b[0m     messages\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_messages_for_qa_type(qa_type, text, validated_num_questions),\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_chat_completion_params,\n\u001b[1;32m    431\u001b[0m )\n\u001b[0;32m--> 432\u001b[0m questions, answers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_qa_from_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(questions) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(answers), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_PARSING_ERR_UNEQUAL_QA\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m qa_type \u001b[38;5;241m==\u001b[39m QAType\u001b[38;5;241m.\u001b[39mCONVERSATION:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.12.8/lib/python3.12/site-packages/azure/ai/generative/synthetic/qa.py:279\u001b[0m, in \u001b[0;36mQADataGenerator._parse_qa_from_response\u001b[0;34m(self, response_text)\u001b[0m\n\u001b[1;32m    277\u001b[0m     last_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Q or A spread across multiple lines\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m last_updated \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_PARSING_ERR_FIRST_LINE\n\u001b[1;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m last_updated \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    281\u001b[0m         questions[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m line\n",
      "\u001b[0;31mAssertionError\u001b[0m: Parsing error: First line must be a question"
     ]
    }
   ],
   "source": [
    "input_batch = mixed_chunks + text_chunks + image_summaries # type: ignore\n",
    "results = await asyncio.gather(*[generate_async(text) for text in input_batch], return_exceptions=True)\n",
    "\n",
    "question_answer_list = []\n",
    "for result in results:\n",
    "    if isinstance(result, Exception):\n",
    "        raise result  # exception raised inside generate_async()\n",
    "    question_answer_list.append(result[\"question_answers\"])\n",
    "\n",
    "print(\"Successfully generated QAs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_answer_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. jsonl形式で保存\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from util.common_utils import convert_to_oai_format, convert_to_jsonl_format, save_jsonl\n",
    "\n",
    "output_dir = './dataset'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "system_prompt_msg = f\"\"\"You are the SME (Subject Matter Expert) in {DOMAIN}. Please answer the questions accurately. If the question is in {LANGUAGE}, write your answer in {LANGUAGE}.\"\"\"\n",
    "\n",
    "save_filename = \"imagenet-training-summary\"\n",
    "oai_qa_pair = convert_to_oai_format(question_answer_list, system_prompt_msg=system_prompt_msg)\n",
    "\n",
    "save_jsonl(oai_qa_pair, f\"{output_dir}/{save_filename}-oai.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AI Foundry での評価用にjsonl形式で保存\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for qa in question_answer_list:\n",
    "    qa_pair = convert_to_jsonl_format(qa)\n",
    "    print(qa_pair)\n",
    "    save_jsonl(qa_pair, f\"{output_dir}/{save_filename}-eval.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## クリーンアップ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf {splitted_raw_data_dir} pdf_image_tmp pdf_mixed_tmp outputs_tmp images {output_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
